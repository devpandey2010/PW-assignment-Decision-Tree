{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4upwrW4WGmFjLNScLdtrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devpandey2010/PW-assignment-Decision-Tree/blob/main/Statistics_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU6-2g_93rT2"
      },
      "outputs": [],
      "source": [
        "'''1. Explain the different types of data (qualitative and quantitative) and provide examples of each. Discuss\n",
        "nominal, ordinal, interval, and ratio scales.\n",
        ">>Data are of two types Quantitative data(Numerical data) and Qualitative data (Non-numreic data).\n",
        "\n",
        "Quantative data are of two types\n",
        "1.Discrete data->A data which takes only discrete values by saying discrete i mean a whole value for example if i number of student in a class it would be 1,2,3,4etc a whole number not a fraction or decimal value.\n",
        "\n",
        "2.Continous data-> A data which can take any numeric value including fraction and decimal example height of a person,area of the house etc\n",
        "\n",
        "Qualitative data>> It is a non-numeric  data  a categorical data but machine learning is learning from pattern and categorical data dont have any pattern so it is required to encode the categorical data.\n",
        "\n",
        "Qualitative data are of two types or we can say encoding is done by keeping watch of the nature of categorical data\n",
        "\n",
        "1.Nominal data>>Nominal data is a type of categorical data where it dont have any order the data are independent of each other and it can be encoded simply by mapping different category with different numbers without thinking\n",
        "about the order.\n",
        "\n",
        "2.Ordinal data>> Ordinal data have order for eg rank first means it should has higher rank similarly second rank so here we encode by maintaining the order for first we map it with 1 second with 2 and third with 3 so an order follows in this data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2.What are the measures of central tendency, and when should you use each? Discuss the mean, median,\n",
        "and mode with examples and situations where each is appropriate.\n",
        "\n",
        ">>1.Measure of Central tendency>>It provides you the representative number of whole dataset which tells that dataset is being revolved around that center value.Like planets revolve around sun the whole dataset will revolve around the centre values.\n",
        "Measure of Central tendency>>\n",
        "1.Mean>> Mean is being referred to central value or the average value .But mean are of three types.Arithematic mean,geometric mean,and harmonic mean.\n",
        "Before going to each type lets know what mean actually represent in tha data\n",
        "Arithmetic mean is the value that minimizes the sum of squared distance from itself\n",
        "athematically,\n",
        "Median\n",
        "\"Median is a statistical measure of central tendency that identifies the middle value in a sorted dataset. It's especially useful when the data contains outliers or is skewed, because unlike the mean, the median is not affected by extreme values. For an odd number of values, it picks the exact middle; for even numbers, it averages the two middle values.\"\n",
        " Why is mean still widely used ‚Äî even though median is better with outliers?\n",
        "\n",
        "Mean is easy to compute, especially on large datasets. It requires only summing and dividing ‚Äî no sorting like median.\n",
        "For streaming data or distributed systems (like Spark, SQL, etc.), mean is faster and memory-efficient.\n",
        " 2. Mean is Differentiable ‚Äî Median is Not\n",
        "This is crucial for machine learning and deep learning:\n",
        "The mean function is smooth and differentiable, which is necessary for gradient descent algorithms to optimize loss functions.\n",
        "\n",
        "\n",
        "In contrast, median introduces non-differentiable points, making it hard to optimize using calculus-based methods.\n",
        "\n",
        "\n",
        " That‚Äôs why mean squared error (MSE) and cross-entropy loss (which are based on mean) are so widely used in regression and classification.\n",
        "\n",
        " 3. Mean Uses All Data Points\n",
        "Mean considers every value in the dataset, so it's sensitive to the full distribution of the data.\n",
        "This can be a strength when your data is normally distributed (symmetric, no outliers).\n",
        "\n",
        "\n",
        "It captures more variation, which is useful for statistical modeling and inference.\n",
        "It only tells where the center is, but doesn‚Äôt use any other data.\n",
        "\n",
        "\n",
        "This makes mean + standard deviation a better pair when the goal is to analyze variability.\n",
        "\n",
        "Mode\n",
        "Mode is the value or category that appears most frequently in a dataset. It‚Äôs particularly useful for categorical data, where other measures like mean or median aren‚Äôt applicable. In numerical datasets, it helps identify repeated values. Mode is robust to outliers and can be unimodal, bimodal, or even multimodal depending on the distribution.‚Äù\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SGu8m_Sx3vb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Explain the concept of dispersion. How do variance and standard deviation measure the spread of data?\n",
        "\"\"\">>‚ÄúYou can think of the mean or median as your hand holding a rope,\n",
        "and each data point as a stone tied to the end.\n",
        "The rope's length reflects how far the data points are from the center ‚Äî just like standard deviation or variance measures how far the data points spread from the mean.\n",
        " A shorter rope means tightly clustered data; a longer rope means widely spread data.‚Äù\n",
        "\n",
        "‚ÄúVariance measures the average of squared deviations from the mean,\n",
        "and it comes in two types: population variance (divided by NNN) and sample variance (divided by n‚àí1n‚àí1n‚àí1).\n",
        "It‚Äôs sensitive to outliers because squaring exaggerates large differences,\n",
        "but this also makes it differentiable ‚Äî which is why variance-based losses like MSE are used in machine learning.\n",
        "A downside of variance is that it‚Äôs expressed in squared units, making it harder to interpret directly compared to standard deviation.\n",
        "\n",
        "‚ÄúWhen we calculate variance from a sample (not full data), dividing by nnn usually gives a value that is too small.\n",
        "To fix this, we divide by n‚àí1n - 1n‚àí1, which slightly increases the result and makes it more accurate.\n",
        "This is called Bessel‚Äôs Correction. Using n‚àí1n - 1n‚àí1 gives us an unbiased estimate ‚Äî meaning, it gives us the right value on average if we repeat the sampling many times.‚Äù\n",
        "\n",
        "Standard deviation is a measure of how much the values in a dataset vary from the mean.\n",
        "It is calculated as the square root of variance, which makes it easier to interpret because it has the same unit as the original data.\n",
        "For example, if your data is in kilograms, the standard deviation will also be in kilograms, unlike variance which would be in kg¬≤.\n",
        "However, standard deviation is still sensitive to outliers, since it's based on squared deviations ‚Äî so large differences from the mean can still heavily influence it.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPm0E0KU3ve6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What is a box plot, and what can it tell you about the distribution of data?\n",
        "'''>>Box plot is the statistical plotting where you plot the data in quartiles and it helps in outlier detection"
      ],
      "metadata": {
        "id": "BaFkBzws3viF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Discuss the role of random sampling in making inferences about populations\n",
        "'''>>echnique\n",
        "1.Simple random sampling  >>Simple Random Sampling (SRS) is a sampling technique in which every data point (or unit) in the population has an equal and independent chance of being selected.\n",
        "There are two key assumptions:\n",
        "Every unit has equal probability.\n",
        "\n",
        "\n",
        "Each selection is independent of the others.\n",
        "\n",
        "\n",
        "Suppose we're conducting a nationwide survey and using simple random sampling. Since Uttar Pradesh has the largest population, people from UP will naturally be more likely to appear in the sample simply because there are more people from there in the total population ‚Äî not because of a flaw in SRS.\n",
        "But if your goal is to ensure equal representation across states, then SRS is not ideal. A better method would be stratified sampling, where each state is treated as a stratum and sampled proportionally or equally\n",
        "Suppose you have a dataset of:\n",
        "900 spam emails\n",
        "\n",
        "\n",
        "100 non-spam emails\n",
        "\n",
        "\n",
        "If you apply simple random sampling, a sample of 100 might contain almost all spam emails and very few or zero non-spam. That hurts model learning and evaluation.\n",
        "\n",
        "2.Stratified Sampling>>\"Stratified sampling is a method where the population is divided into homogeneous groups called strata, based on a specific feature (like gender, state, or class label). Then, random sampling is done within each stratum to ensure that all important subgroups are proportionally or equally represented in the final sample. This method solves the problem of certain groups being underrepresented in simple random sampling.\"\n",
        "\n",
        "3.Cluster Sampling>>\"In stratified sampling, we divide the population into meaningful subgroups (strata) and sample from each to ensure representation. In cluster sampling, we divide the population into mini-population groups (clusters), randomly pick a few clusters, and sample all or some items from only those. Stratified improves accuracy and representation, while cluster sampling improves efficiency and feasibility.\"\n",
        "4.systematic sampling>>Systematic sampling is a method where we select items at regular intervals from an ordered population list. The first item is chosen randomly, and subsequent items are selected using a fixed step size kkk. This method is simple, fast, and ensures uniform coverage ‚Äî but it can introduce bias if there‚Äôs a hidden pattern in the data aligned with the interval.\n"
      ],
      "metadata": {
        "id": "WoLY-ilD3vlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Explain the concept of skewness and its types. How does skewness affect the interpretation of data?\n",
        "'''>>Skewness describes the asymmetry of a data distribution.\n",
        "In symmetric distributions, the mean, median, and mode are equal.\n",
        " In right-skewed data, outliers on the high end pull the mean higher than the median and mode.\n",
        "  In left-skewed data, outliers on the low end pull the mean lower.\n",
        "  Skewness is often caused by outliers or natural boundaries in the data.\n",
        "  It's important to detect because it can affect model performance and how we summarize the data.‚Äù\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qzot0UDa3voQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. What is the interquartile range (IQR), and how is it used to detect outliers?\n",
        "'''>>‚ÄúThe range measures the total spread of data but is highly sensitive to outliers,\n",
        "since it only considers the min and max values. Percentiles, on the other hand,\n",
        "divide data into 100 equal parts and give a clearer view of distribution.\n",
        "For instance, the 75th percentile means 75% of the data lies below that value.\n",
        "The IQR, which is the difference between the 75th and 25th percentiles, is often used instead of range because it‚Äôs more robust to outliers.‚Äù\n",
        "\n",
        "‚ÄúQuartiles are cut-off points that divide a dataset into four equal parts.\n",
        "These cuts ‚Äî Q1, Q2, and Q3 ‚Äî give us insight into the distribution of the data.\n",
        "Using these quartiles, we can calculate the Interquartile Range (IQR), which is useful for detecting and handling outliers.‚Äù\n",
        "\n",
        "\"Interquartile Range (IQR) measures the spread of the middle 50% of data by subtracting the 25th percentile (Q1) from the 75th percentile (Q3).\n",
        "It‚Äôs a robust measure of dispersion, unaffected by outliers, making it especially useful for exploratory data analysis and outlier detection.\n",
        "In contrast to standard deviation, which uses all data points, IQR focuses only on the core distribution, giving a more stable view in skewed or messy data.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hvjZO8AO3vrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Discuss the conditions under which the binomial distribution is used.\n",
        "\n",
        "'''>>Condition of Bernoulli (Very important)\n",
        "\n",
        ".Number of trials should be finite.\n",
        ".Each trials should be independent.\n",
        ".Output of each trail should be two\n",
        ".probability of same outcome in each trial should be same\n",
        "\n",
        " What is Bernoulli Distribution?\n",
        "The Bernoulli distribution models binary outcomes:\n",
        "X‚àà{0,1}X \\in \\{0, 1\\}X‚àà{0,1}\n",
        "With:\n",
        "P(X=1)=pP(X = 1) = pP(X=1)=p\n",
        "\n",
        "\n",
        "P(X=0)=1‚àípP(X = 0) = 1 - pP(X=0)=1‚àíp\n",
        "\n",
        "\n",
        " PMF (probability mass function):\n",
        "P(X=x)=px(1‚àíp)1‚àíx,x‚àà{0,1}P(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}P(X=x)=px(1‚àíp)1‚àíx,x‚àà{0,1}\n",
        "It is the simplest discrete probability distribution ‚Äî used when we have only two outcomes: success/failure, yes/no, positive/negative, etc\n"
      ],
      "metadata": {
        "id": "FvAKZKTO3vuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Explain the properties of the normal distribution and the empirical rule (68-95-99.7 rule).\n",
        "'''the normal distribution models continuous data that tends to cluster around a central value (mean), with symmetric spread on both sides.\n",
        "It is the famous bell-shaped curve ‚Äî used everywhere from exam scores to natural measurements to ML algorithms.\n",
        "For any normal distribution:\n",
        "Range Around Mean\n",
        "% of Data\n",
        "Œº¬±1œÉ\\mu \\pm 1\\sigmaŒº¬±1œÉ\n",
        "~68%\n",
        "Œº¬±2œÉ\\mu \\pm 2\\sigmaŒº¬±2œÉ\n",
        "~95%\n",
        "Œº¬±3œÉ\\mu \\pm 3\\sigmaŒº¬±3œÉ\n",
        "~99.7%\n",
        " Example:\n",
        "Let‚Äôs say:\n",
        "\n",
        "ùëã\n",
        "‚àº\n",
        "ùëÅ\n",
        "(\n",
        "100\n",
        ",\n",
        "15\n",
        "2\n",
        ")\n",
        "X‚àºN(100,15\n",
        "2\n",
        " )\n",
        "\n",
        "This could represent exam scores\n",
        "\n",
        "Mean score: 100\n",
        "\n",
        "Std deviation: 15\n",
        "\n",
        "Then:\n",
        "\n",
        "68% of students score between 85 and 115\n",
        "\n",
        "95% score between 70 and 130\n",
        "A special case of normal distribution with:\n",
        "\n",
        "ùúá\n",
        "=\n",
        "0\n",
        "Œº=0\n",
        "\n",
        "ùúé\n",
        "=\n",
        "1\n",
        "œÉ=1\n",
        "\n",
        "We call it Z-distribution and denote the variable as\n",
        "ùëç\n",
        "‚àº\n",
        "ùëÅ\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "Z‚àºN(0,1)\n",
        "\n",
        "To convert any normal variable to standard normal:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "z=\n",
        "œÉ\n",
        "x‚àíŒº\n",
        "The normal distribution is a bell-shaped curve where values are centered around the mean. It‚Äôs symmetric, mathematically beautiful, and used everywhere in machine learning, deep learning, and statistics. You can define any normal distribution by its mean and standard deviation, and the curve tells you the likelihood of a value occurring in a continuous space.\n",
        "The standard normal distribution is a normalized form of the normal distribution with a mean of 0 and standard deviation of 1. It allows us to compare data across different normal distributions by removing the influence of different means and scales.\n",
        "Using the Z-score, we convert any value into a common scale that tells us how far (in standard deviations) it is from the mean. This is essential in comparison, normalization, and many statistical tests and machine learning tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "0C8jTaE5jC1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Provide a real-life example of a Poisson process and calculate the probability for a specific event\n",
        "'''The Poisson Process is the continuous-time version of the Poisson distribution.\n",
        "\n",
        "It‚Äôs the basis for:\n",
        "\n",
        "Modeling arrival times\n",
        "\n",
        "Inter-arrival time distributions (which follow Exponential distribution)\n",
        "Used in:\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Network traffic analysis\n",
        "\n",
        "Healthcare event timing (e.g., seizures, admissions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nc-ZkR8EjCfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Explain what a random variable is and differentiate between discrete and continuous random variables\n",
        "'''>>A random variable is a function that assigns a numerical value to each outcome of a random experiment. For example, when rolling a die, the outcome is uncertain ‚Äî you might get any number from 1 to 6.\n",
        "Each outcome is associated with a probability, so the number you get is a random variable. It‚Äôs not fixed ‚Äî it depends on chance.‚Äù"
      ],
      "metadata": {
        "id": "UiPYudXT3vxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Provide an example dataset, calculate both covariance and correlation, and interpret the results.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "data=sns.load_dataset(\"tips\")\n",
        "df=pd.DataFrame(data)\n",
        "df\n",
        "df.head()\n",
        "print(df.corr(\"pearson\",numeric_only=True))\n",
        "print(df.cov(numeric_only=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "bTRxFx9C3v0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689bd1e3-7638-4795-f915-7d4b94956ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            total_bill       tip      size\n",
            "total_bill    1.000000  0.675734  0.598315\n",
            "tip           0.675734  1.000000  0.489299\n",
            "size          0.598315  0.489299  1.000000\n",
            "            total_bill       tip      size\n",
            "total_bill   79.252939  8.323502  5.065983\n",
            "tip           8.323502  1.914455  0.643906\n",
            "size          5.065983  0.643906  0.904591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hozr6tL3v37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruAPQlG93v7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgeK4m_a3v95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCs8AGI63wBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TqdA61X3wFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "18xzha5s3wIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQ4OQ-z23wL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZUP35IH3wPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}